{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains decision tree and adaboost implementation\n",
    "Links to download dataset\n",
    "1. https://www.kaggle.com/blastchar/telco-customer-churn\n",
    "2. https://archive.ics.uci.edu/ml/datasets/adult\n",
    "3. https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict  \n",
    "from queue import Queue \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import math\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(lst):\n",
    "  lst = np.array(lst)\n",
    "  uniq = np.unique(lst)\n",
    "  indx =  np.arange(len( uniq)) \n",
    "  for i in indx:\n",
    "    lst[ lst == uniq[i] ] = str(i)\n",
    "\n",
    "  return [str(i) for i in lst]\n",
    "\n",
    "def binning(lst, quantile = None):\n",
    "  # labels= [str(i) for i in np.arange(quantile)]\n",
    "  if quantile is None:\n",
    "    quantile = 4\n",
    "  try:\n",
    "    tmpLst = pd.qcut(lst, q = quantile)\n",
    "  except:\n",
    "    tmpLst = pd.cut(lst, quantile)\n",
    "  return tmpLst\n",
    "\n",
    "\n",
    "def castToType(x):\n",
    "    try:\n",
    "        return x.astype('float').astype('Int64') #pd 0.24+ \n",
    "    except:\n",
    "        try:\n",
    "            return x.astype('float')\n",
    "        except:\n",
    "            return x\n",
    "\n",
    "\n",
    "def measure_performance(y_test, y_pred):\n",
    "    labels = list(set(y_test))\n",
    "    print('\\t\\t\\t', labels)\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN) * 100\n",
    "    print('True positive rate ', TPR)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) * 100\n",
    "    print('True negative rate ', TNR)\n",
    "\n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP) * 100\n",
    "    print('Positive predictive value ', PPV)\n",
    "\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN) * 100\n",
    "    # print('True positive rate ', TPR)\n",
    "\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN) * 100\n",
    "    # print('True positive rate ', TPR)\n",
    "\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN) * 100\n",
    "\n",
    "\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP) * 100\n",
    "    print('False discovery rate ', FDR)\n",
    "\n",
    "    F1_score = 2 * PPV * TPR / (PPV + TPR) \n",
    "    print('F1 score ', F1_score)\n",
    "\n",
    "\n",
    "    print(metrics.accuracy_score(y_test,y_pred) * 100)\n",
    "\n",
    "\n",
    "def isNumeric(num):\n",
    "    return isinstance(num, numbers.Number)\n",
    "\n",
    "missing_values = [\"n/a\", \"na\", \"--\",\"NA\",\"N/A\",\"?\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import numbers\n",
    "import random\n",
    "MX_CONT_SIZE = 8000\n",
    "PER_ROW_MEDIAN_ENTRY = 2\n",
    "\n",
    "def targetEntropy( lst):\n",
    "        lst = np.array(lst)\n",
    "        values, counts = np.unique(lst, return_counts=True)\n",
    "        total = np.sum(counts)\n",
    "        entr = counts/total * np.log2(counts/total) \n",
    "        return -np.sum(entr)\n",
    "\n",
    "def isNumeric(num):\n",
    "    return isinstance(num, numbers.Number)\n",
    "\n",
    "\n",
    "class Question:\n",
    "\n",
    "    def __init__(self, colForQuest, isNum, valLst=None, splitNum=None):\n",
    "        self.colForQuest = colForQuest\n",
    "        self.isNum = isNum\n",
    "        self.valLst = valLst\n",
    "        self.splitNum = splitNum\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.colForQuest) + \" \" + str(self.isNum)  + \" \"+ str(self.valLst) + \" \" + str(self.splitNum) \n",
    "        \n",
    "    def giveMatchingChild(self, data):\n",
    "        if self.isNum:\n",
    "            if data[self.colForQuest] <= self.splitNum:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        else:\n",
    "            \n",
    "            for i in range(len(self.valLst)):\n",
    "                if self.valLst[i] == data[self.colForQuest]:\n",
    "                    return i\n",
    "\n",
    "            return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Node:\n",
    "\n",
    "    def __init__(self, dataset, depth): \n",
    "        self.depth = depth \n",
    "        colList = dataset[0,:]\n",
    "        tmpDataset = dataset[1:,:]\n",
    "        self.entropy = targetEntropy(tmpDataset[:,-1])\n",
    "        mxInfoGain = -10000\n",
    "        splitIndx = 0\n",
    "        self.children = []\n",
    "        self.question = None\n",
    "        info = 0\n",
    "        self.splitNum = -1\n",
    "        tmpSplitVal = -1\n",
    "\n",
    "        self.isLeaf = ( depth == 0 ) or (self.entropy == 0) or (len(tmpDataset[0]) == 1 )\n",
    "\n",
    "        \n",
    "        if not self.isLeaf:\n",
    "            for i in range(len(tmpDataset[0])-1):\n",
    "\n",
    "                if isNumeric(tmpDataset[0][i]):\n",
    "                    info, tmpSplitVal = self.nodeEntropy(tmpDataset[:,i], tmpDataset[:, -1])\n",
    "                else:\n",
    "                    info = self.nodeEntropy(tmpDataset[:,i], tmpDataset[:, -1])\n",
    "                infoGain = self.entropy - info\n",
    "                if mxInfoGain < infoGain:\n",
    "                    if isNumeric(tmpDataset[0][i]):\n",
    "                        self.splitNum = tmpSplitVal\n",
    "                    mxInfoGain, splitIndx = infoGain, i\n",
    "\n",
    "            if mxInfoGain <= 0:\n",
    "                self.isLeaf = True\n",
    "                \n",
    "            else:\n",
    "                isNum = isNumeric( tmpDataset[0][splitIndx])\n",
    "                self.valLst = []\n",
    "\n",
    "                idxs = list( range(len(tmpDataset[0])))\n",
    "                idxs.pop(splitIndx) #this removes elements from the list\n",
    "            \n",
    "\n",
    "                if isNum:\n",
    "\n",
    "                    if not self.isLeaf:\n",
    "                        \n",
    "                        self.children.append(Node( np.vstack((colList, tmpDataset[ tmpDataset[:,splitIndx] <= self.splitNum]))[:,idxs], depth - 1 )) \n",
    "                        self.children.append(Node( np.vstack( (colList,tmpDataset[ tmpDataset[:,splitIndx] > self.splitNum]) )[:, idxs], depth - 1 )) \n",
    "\n",
    "                else:\n",
    "                    self.valLst = np.unique(tmpDataset[:, splitIndx])\n",
    "\n",
    "                    if not self.isLeaf:\n",
    "                        for i in self.valLst:\n",
    "                            self.children.append(Node( np.vstack((colList,tmpDataset[ tmpDataset[:,splitIndx]  == i]) )[:, idxs], depth - 1 )) \n",
    "\n",
    "                self.question = Question(colList[splitIndx] , isNum, valLst=self.valLst, splitNum= self.splitNum ) \n",
    "\n",
    "        if self.isLeaf:\n",
    "            targetCol = np.array(tmpDataset[:,-1], dtype='O') \n",
    "            val, cnt = np.unique( targetCol, return_counts=True)\n",
    "            idx = random.choice(np.argwhere(cnt == max(cnt)))[0]\n",
    "            self.predictedVal =  val[ idx] \n",
    "    \n",
    "    def nodeEntropy(self, splitByCol, targetCol):\n",
    "        isNum = isNumeric(splitByCol[0])\n",
    "\n",
    "        combindedLst = np.array( list(zip(splitByCol, targetCol)), dtype='O')\n",
    "        weightLst = []\n",
    "        total = len(targetCol)\n",
    "\n",
    "        if isNum:\n",
    "            combindedLst = combindedLst[ combindedLst[:,0].argsort()]\n",
    "            vals = np.unique(combindedLst[:,0]) \n",
    "            vals = vals[:-1] + np.diff(vals)/2.0 \n",
    "            \n",
    "            # for time optimization, build the median list of every consecutive 4/5 elements of sorted array  \n",
    "            # print(vals.shape[0])\n",
    "            # if vals.shape[0] > MX_CONT_SIZE:\n",
    "            #     vals.resize(vals.shape[0]//PER_ROW_MEDIAN_ENTRY + 1 , PER_ROW_MEDIAN_ENTRY)\n",
    "            #     vals = np.median(vals, axis=1)  \n",
    "            #     print(vals.shape)\n",
    "\n",
    "            leastEntropy,splitVal = 1000000, -10\n",
    "\n",
    "            for i in vals:\n",
    "                entrLst = []\n",
    "                weightLst = []\n",
    "\n",
    "                for j in range(2):\n",
    "\n",
    "                    tmpList = []\n",
    "                    indx = np.searchsorted( combindedLst[:,0], j, side='right') \n",
    "                    if j == 0:\n",
    "                        tmpList = combindedLst[:indx, :]\n",
    "                    else:\n",
    "                        tmpList = combindedLst[indx:, :]\n",
    "\n",
    "                    val, cnt = np.unique(tmpList[:,1], return_counts=True)\n",
    "                    tmpCnt = np.sum(cnt)\n",
    "                    weightLst.append(tmpCnt)\n",
    "                    entrLst.append(-np.sum( cnt/tmpCnt * np.log2(cnt/tmpCnt) ) )  \n",
    "                    \n",
    "                entropy = np.sum(  np.array(entrLst) * np.array(weightLst)/total )\n",
    "                if entropy < leastEntropy:\n",
    "                    leastEntropy, splitVal = entropy, i\n",
    "            return leastEntropy, splitVal\n",
    "        else:\n",
    "            values, counts = np.unique(splitByCol, return_counts=True)\n",
    "            entrLst = []\n",
    "            weightLst = []\n",
    "\n",
    "            for i in values:\n",
    "                tmpList = combindedLst[combindedLst[:,0] == i]\n",
    "                val, cnt = np.unique(tmpList[:,1], return_counts=True)\n",
    "                tmpCnt = np.sum(cnt)\n",
    "                weightLst.append(tmpCnt)\n",
    "                entrLst.append(-np.sum( cnt/tmpCnt * np.log2(cnt/tmpCnt) ) )  \n",
    "            \n",
    "            return np.sum( np.array(entrLst) * np.array(weightLst)/total)\n",
    "\n",
    "    def predict(self, data):\n",
    "\n",
    "        if self.isLeaf:\n",
    "            return self.predictedVal # highest count target column value \n",
    "        else:\n",
    "            childIndx = self.question.giveMatchingChild(data)\n",
    "            return self.children[childIndx].predict(data) \n",
    "            \n",
    "    def print(self):\n",
    "      if self.isLeaf:\n",
    "        print('(leaf) Entropy=', self.entropy,' Highest Count=', self.predictedVal, ' Height=',self.depth,'\\t',end=' ')\n",
    "      else:\n",
    "        print('Entropy=', self.entropy, ' Split on=', self.question.colForQuest,' Height=',self.depth,'\\t',end=' ')\n",
    "    \n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, depth = 1000):\n",
    "        self.entropy = 0\n",
    "        self.depth = depth \n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        a = np.array(x_train, dtype='O')\n",
    "        b = np.array(y_train, dtype='O')\n",
    "        self.dataset = np.column_stack((a,b))\n",
    "        colNo = np.array(list(range( self.dataset.shape[1] )), dtype='O')\n",
    "        self.dataset = np.vstack((colNo, self.dataset))\n",
    "        self.entropy = targetEntropy(y_train)\n",
    "        self.rootNode = Node(self.dataset, self.depth)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        y_pred = []\n",
    "        for data in  x_test:\n",
    "            y_pred.append(self.rootNode.predict(data))\n",
    "        return y_pred\n",
    "\n",
    "    def print(self):\n",
    "      q = Queue()\n",
    "      q.put(self.rootNode)\n",
    "      depth = self.rootNode.depth\n",
    "      while not q.empty()  :\n",
    "        node = q.get()     \n",
    "        print(node.depth*'\\t',end=' ')\n",
    "        node.print()\n",
    "        print('')\n",
    "\n",
    "\n",
    "        for child in node.children:\n",
    "          q.put(child)\n",
    "          depth = child.depth\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict  \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Adaboost:\n",
    "    \n",
    "    def __init__(self, numOfHypothesis):\n",
    "        self.hypoNum = numOfHypothesis\n",
    "        self.hypoList = [] \n",
    "        self.hypoWeight = [1] * numOfHypothesis\n",
    "\n",
    "    def normalize(self, lst):\n",
    "        lst = np.array(lst)\n",
    "        sum = np.sum(lst)\n",
    "        return lst/sum\n",
    "\n",
    "    def getResampleData(self):\n",
    "        chosenIdxs = random.choices( population= list( range( len(self.dataset) ) ), weights=self.sampleWeight, k=len(self.dataset) )\n",
    "        return self.dataset[chosenIdxs]\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        x_train = np.array(x_train, dtype=\"O\")\n",
    "        y_train = np.array(y_train,  dtype=\"O\")\n",
    "        self.dataset = np.column_stack((x_train, y_train))\n",
    "        self.sampleWeight = [1/len(self.dataset)] * len(self.dataset)\n",
    "\n",
    "        k = 0\n",
    "        while k < self.hypoNum:\n",
    "            dtc = DecisionTree(1)\n",
    "            data = self.getResampleData()\n",
    "\n",
    "            # dtc = DecisionTreeClassifier(random_state=0, max_depth=1)\n",
    "            # data = self.dataset\n",
    "            tmpHypoResult = self.hypothesisResult( dtc, data )\n",
    "            error = self.getError(tmpHypoResult, y_train)\n",
    "\n",
    "            if error > 0.5:\n",
    "                continue\n",
    "            for j in range(len(y_train)):\n",
    "                if tmpHypoResult[j] == y_train[j]:\n",
    "                    self.sampleWeight[j] = self.sampleWeight[j] * error/(1 - error)\n",
    "            \n",
    "            self.sampleWeight = self.normalize(self.sampleWeight)\n",
    "            self.hypoList.append(dtc)\n",
    "            self.hypoWeight[k] = math.log2( (1-error)/ error )\n",
    "            k = k + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def getError(self, y_pred, y_test):\n",
    "        error = 0\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] != y_test[i]:\n",
    "                error = error + self.sampleWeight[i]\n",
    "        return error\n",
    "\n",
    "    def hypothesisResult(self, dtc, resampleData ):\n",
    "        dtc.fit(resampleData[:,:-1], resampleData[:,-1] )\n",
    "\n",
    "        return dtc.predict(self.dataset[:,:-1])\n",
    " \n",
    "\n",
    "    def predict(self, x_test):\n",
    "        y_predLst = []\n",
    "        y_test = []\n",
    "        for hypo in self.hypoList:\n",
    "            y_test.append( hypo.predict(x_test) ) \n",
    "            \n",
    "        y_test = np.array(y_test)\n",
    "        for i in range(len(x_test)):\n",
    "            y_pred = y_test[:,i]\n",
    "            voting = defaultdict(int)\n",
    "            for k in range( self.hypoNum ):\n",
    "                voting[ y_pred[k] ] += self.hypoWeight[k]\n",
    "            y_predLst.append( max(voting, key=voting.get) ) \n",
    "        \n",
    "        return y_predLst\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Tuning \n",
    "Only tree-depth is being optimized here.\n",
    "Things like max_split/number of leaves are also subjet to optmization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    return metrics.accuracy_score(y_test,model.predict(X_test))\n",
    "\n",
    "#Now only finds out the optimal tree depth \n",
    "def hyperParaTuning():\n",
    "  from sklearn.model_selection import StratifiedKFold\n",
    "  folds = StratifiedKFold(n_splits=3)\n",
    "\n",
    "  scores_tree = []\n",
    "  best_height, best_score = 5, 0\n",
    "  \n",
    "  for h in random.sample( list(range(3,13)), k = 5):\n",
    "    for train_index, test_index in folds.split(x_train, y_train):\n",
    "        _X_train, _X_test, _Y_train, _Y_test = [x_train[i] for i in train_index], \\\n",
    "                                                [x_train[i] for i in test_index], \\\n",
    "                                                  [y_train[i] for i in train_index],\\\n",
    "                                                  [y_train[i] for i in test_index] \n",
    "        scores_tree.append(get_score(DecisionTree(5), _X_train, _X_test, _Y_train, _Y_test))  \n",
    "    if np.average(scores_tree) > best_score:\n",
    "      best_score, best_height = np.average(scores_tree) , h\n",
    "\n",
    "  return best_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telco Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('telco', na_values = missing_values)\n",
    "# df = df.drop(columns=['gender'])\n",
    "\n",
    "# colToCategorise =  ['SeniorCitizen','Partner','Dependents', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', \n",
    "#                  'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod' ]\n",
    "# colToBinning = ['tenure',\t'MonthlyCharges', 'TotalCharges']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('adult.data', header=None, na_values = missing_values, skip_blank_lines=True)\n",
    "df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation',\n",
    "              'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'salary']\n",
    "\n",
    "colToCategorise = ['workclass', 'education',  'marital-status', 'occupation',\n",
    "              'relationship', 'race', 'sex', 'native-country']\n",
    "colToBinning = ['age' , 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit-Card Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('creditcard', na_values = missing_values, skip_blank_lines=True)\n",
    "# pos_sample = df[df.iloc[:,-1] == 1]\n",
    "# neg_sample = df[df.iloc[:,-1] == 0].sample(n=20000, random_state=0)\n",
    "# df = shuffle(pd.concat( (pos_sample,neg_sample), axis=0), random_state=0) \n",
    "# df.reset_index(drop=True)\n",
    "\n",
    "# colToCategorise = []\n",
    "# colToBinning = set(df.columns[:-1]) - set(['V1','V2']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing + Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "print('Data processing start')\n",
    "# -----------------------------------Data cleaning--------------------------------\n",
    "df = df.applymap(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "\n",
    "df.replace(r'^[\\s?]*$', np.nan, regex=True, inplace = True)\n",
    "\n",
    "for col in df.columns:\n",
    "    if len(df[col].unique()) == 1:\n",
    "        print(col)\n",
    "        df.drop(col,inplace=True,axis=1)\n",
    "\n",
    "for col in df.columns:\n",
    "    if len(df[col].unique()) == len(df[col]):\n",
    "        print(col)\n",
    "        df.drop(col,inplace=True,axis=1)\n",
    "\n",
    "df = df.apply(castToType)\n",
    "for col in df.columns:\n",
    "\n",
    "    if df[col].isnull().any() :\n",
    "        if np.issubdtype(df[col].dtype, np.number):\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        # else:\n",
    "        #   #Max fill function for categorical columns\n",
    "        #   data['column_name'].fillna(data['column_name'].value_counts()\n",
    "        #   .idxmax(), inplace=True)\n",
    "\n",
    "#drop all columns with 60% or more null values \n",
    "nullThreshold = 0.7\n",
    "\n",
    "df.dropna(thresh=df.shape[0]*nullThreshold,how='all',axis=1, inplace= True)\n",
    "df.dropna(axis = 0, inplace = True)\n",
    "# -----------------------------------Data cleaning--------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------Categorization/Binning------------------------------\n",
    "\n",
    "for col in colToCategorise:\n",
    "  df[col] = categorize(df[col])\n",
    "\n",
    "for col in colToBinning:\n",
    "  df[col] = binning(df[col])\n",
    "# -------------------------------------------Categorization/Binning------------------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df,test_size=0.2, random_state=0)\n",
    "\n",
    "x_train, y_train = train.drop(train.columns[-1], axis=1).values.tolist(),  train[train.columns[-1]].values.tolist()\n",
    "x_test, y_test = test.drop(test.columns[-1], axis=1).values.tolist(),  test[test.columns[-1]].values.tolist()\n",
    "\n",
    "print('Data processing Done')\n",
    "print('Time taken ----', time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_depth = hyperParaTuning()\n",
    "# best_depth = 7 #for telco, adult\n",
    "print(best_depth)\n",
    "\n",
    "dtc = DecisionTree(best_depth)\n",
    "# dtc = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "print('-- Data fitting start')\n",
    "\n",
    "# boost.fit(x_train, y_train)\n",
    "dtc.fit(x_train, y_train)\n",
    "\n",
    "print('Data fitting finished')\n",
    "print('Time taken ----', time.time() - start_time)\n",
    "# dtc.print()\n",
    "print('Data prediction start')\n",
    "start_time = time.time()\n",
    "\n",
    "# y_pred = boost.predict(x_test)\n",
    "y_pred = dtc.predict(x_test)\n",
    "y_train_pred = dtc.predict(x_train)\n",
    "print('Data fitting finished')\n",
    "print('Time taken ----', time.time() - start_time)\n",
    "\n",
    "# print(y_pred)\n",
    "# print(y_test)\n",
    "print('Training set performance')\n",
    "print(metrics.classification_report(y_train,y_train_pred))\n",
    "print(metrics.accuracy_score(y_train,y_train_pred) * 100)\n",
    "# print(metrics.confusion_matrix(y_train, y_train_pred))\n",
    "measure_performance(y_train, y_train_pred)\n",
    "\n",
    "print('Testing set performance')\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.accuracy_score(y_test,y_pred) * 100)\n",
    "# print(metrics.confusion_matrix(y_test, y_pred))\n",
    "measure_performance(y_test, y_pred)\n",
    "# print(calc_perfrom_BinClass(y_test, y_pred))\n",
    "\n",
    "print('Data prediction finished')\n",
    "print('Time taken ----', time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for stump in [5,10,15,20]:\n",
    "  boost = Adaboost(stump)\n",
    "\n",
    "  start_time = time.time()\n",
    "  print('-- Data fitting start')\n",
    "\n",
    "  # boost.fit(x_train, y_train)\n",
    "  boost.fit(x_train, y_train)\n",
    "\n",
    "  print('Data fitting finished')\n",
    "  print('Time taken ----', time.time() - start_time)\n",
    "\n",
    "  print('Data prediction start')\n",
    "  start_time = time.time()\n",
    "\n",
    "  y_pred = boost.predict(x_test)\n",
    "  y_train_pred = boost.predict(x_train)\n",
    "  print('Data fitting finished')\n",
    "  print('Time taken ----', time.time() - start_time)\n",
    "\n",
    "  print('No of stumps ',stump)\n",
    "  print('Training set performance')\n",
    "  print(metrics.accuracy_score(y_train,y_train_pred) * 100)\n",
    "\n",
    "  print('Testing set performance')\n",
    "  print(metrics.accuracy_score(y_test,y_pred) * 100)\n",
    "  print('Data prediction finished')\n",
    "  print('Time taken ----', time.time() - start_time)\n",
    "  print('------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
